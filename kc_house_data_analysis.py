# -*- coding: utf-8 -*-
"""kc_house_data_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E2-pHGuwvrchgH6jf0dUDTiduf7180c8
"""

# Import library for exploring dataset
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df1 = pd.read_csv("/content/house(1).csv")
df2 = pd.read_csv("/content/house(2).csv")

common_column = 'id'
df = pd.merge(df1, df2, on=common_column)
df.replace(['?', '/', '#'], np.nan, inplace=True)

df.head(10)

df.info()

df['yr_renovated'].value_counts()

df.isnull().sum()

df['floors'].value_counts()

# Calculate the mode
mode_val = df['floors'].mode()[0]

# Fill the null values
df['floors'].fillna(mode_val, inplace=True)

df['bedrooms'].value_counts()

# Calculate the mode
mode_val = df['bedrooms'].mode()[0]

# Fill the null values
df['bedrooms'].fillna(mode_val, inplace=True)

df['bathrooms'].value_counts()

# Calculate the mode
mode_val = df['bathrooms'].mode()[0]

# Fill the null values
df['bathrooms'].fillna(mode_val, inplace=True)

df['waterfront'].value_counts()

# Calculate the mode
mode_val = df['waterfront'].mode()[0]

# Fill the null values
df['waterfront'].fillna(mode_val, inplace=True)

df.isnull().any()

df.describe()

# remove id and date features
df.drop(["id"], axis=1, inplace=True)

# defining numerical & categorical columns
numeric_features = [feature for feature in df.columns if df[feature].dtype != 'O']
categorical_features = [feature for feature in df.columns if df[feature].dtype == 'O']

# print columns
print('We have {} numerical features : {}'.format(len(numeric_features), numeric_features))
print('\nWe have {} categorical features : {}'.format(len(categorical_features), categorical_features))



#check the distribution of numerical features in dataset
for feature in numeric_features:
    data=df.copy()
    data[feature].hist(bins=25)
    plt.xlabel(feature)
    plt.ylabel("Count")
    plt.title(feature)
    plt.show()

fig, ax = plt.subplots(figsize=(25, 10))
sns.set_style('whitegrid')
sns.countplot(x='date',data=df,ax=ax)

fig, ax = plt.subplots(figsize=(12, 6))
sns.set_style('whitegrid')
sns.countplot(x='waterfront',data=df,ax=ax)

fig = plt.figure(figsize=(15, 20))

for i in range(0, len(numeric_features)):
    ax = plt.subplot(10, 2, i+1)

    sns.scatterplot(data= df ,x='price', y=numeric_features[i], color='b')
  
    plt.tight_layout()

df['price']=np.log(df.price)

fig, ax = plt.subplots(figsize=(10,6))
sns.set_style('whitegrid')
sns.histplot(x='price', bins=200, kde=True, color = 'b',data=df,ax=ax)

fig, ax = plt.subplots(figsize=(30,8))
sns.set_style('whitegrid')
sns.barplot(x=df.zipcode, y=df.price)

# check for duplicates
df.duplicated().sum()

# drop duplicates
df.drop_duplicates(inplace=True)

# convert date to datetime object
df["date"] = pd.to_datetime(df["date"])

df['zipcode'].value_counts()

df.head(10)



# remove id and date features
df.drop(["date"], axis=1, inplace=True)

df=pd.get_dummies(df,columns=['waterfront'],drop_first=True)

df.columns

# Create boxplots for all numeric columns in the dataset
sns.set(style="whitegrid")
for col in df:
    sns.boxplot(data=df[col], orient="v", palette="Set2")
    plt.title(col)
    plt.show()

# Identify the columns with potential outliers
outlier_cols = ['sqft_basement', 'yr_built','zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15','bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors','condition', 'grade', 'sqft_above']

# Replace outliers with the upper and lower bounds
for col in outlier_cols:
    q1 = df[col].quantile(0.25)
    q3 = df[col].quantile(0.75)
    iqr = q3 - q1
    upper_bound = q3 + 1.5*iqr
    lower_bound = q1 - 1.5*iqr
    df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])
    df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])

# Create boxplots for all numeric columns in the dataset
sns.set(style="whitegrid")
for col in df:
    sns.boxplot(data=df[col], orient="v", palette="Set2")
    plt.title(col)
    plt.show()

df=df.drop('waterfront_1',axis=1)

# normalize numerical features
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
df[["sqft_living", "sqft_lot", "sqft_above", "sqft_basement", "sqft_living15", "sqft_lot15"]] = scaler.fit_transform(df[["sqft_living", "sqft_lot", "sqft_above", "sqft_basement", "sqft_living15", "sqft_lot15"]])

df.head()

df['yr_renovated'].value_counts()



df.corr()

plt.figure(figsize = (15,10))
sns.heatmap(df.corr(), cmap="CMRmap", annot=True)
plt.show()

import numpy as np

class DecisionTreeRegressor:
    def __init__(self, max_depth=None, min_samples_split=2):
        self.max_depth = max_depth#determines the maximum depth of the decision tree that will be constructed
        self.min_samples_split = min_samples_split#specifies the minimum number of samples required to split an internal node
        self.tree = None

    def mean_squared_error(self, y):
        return np.mean((y - np.mean(y)) ** 2)# Calculate mean squared error of targets

    def split_data(self, X, y, feature_idx, threshold):
        left_mask = X[:, feature_idx] <= threshold# find the indices of samples where the feature is less than the threshold value
        right_mask = X[:, feature_idx] > threshold # find the indices of samples where the feature is greater than or equal to the split value
        X_left, y_left = X[left_mask], y[left_mask] #create a new array of input samples for the left node using the left indices, #create a new array of labels for the left node using the left indice
        X_right, y_right = X[right_mask], y[right_mask]# create a new array of input samples for the right node using the right indices,# create a new array of labels for the right node using the right indices
        return X_left, y_left, X_right, y_right# return the new input and label arrays for the left and right nodes

    def find_best_split(self, X, y):
        ''' function to find the best split '''
        best_feature_idx, best_threshold, best_mse = None, None, np.inf
         #ensure that the first value you encounter will be greater than the current maximum value
         # loop over all the features in the dataset
        for feature_idx in range(X.shape[1]):
            # loop over all the unique feature values present in the data
            for threshold in np.unique(X[:, feature_idx]):
              # get current split
                X_left, y_left, X_right, y_right = self.split_data(X, y, feature_idx, threshold)
                #The code checks if the number of samples in the left and right subsets of the current dataset are less than a specified minimum threshold 
                if len(y_left) < self.min_samples_split or len(y_right) < self.min_samples_split:
                    continue
               # The split separates the data into two subsets: one on the left and the other on the right. The mean squared error is calculated for each subset separately using the method mean_squared_error()
                mse_left, mse_right = self.mean_squared_error(y_left), self.mean_squared_error(y_right)
                mse = mse_left + mse_right
                #It compares the MSE for each split to the current best MSE and updates the best feature index, best threshold, and best MSE if the current split has a lower MSE than the current best split.
                if mse < best_mse:
                    best_feature_idx, best_threshold, best_mse = feature_idx, threshold, mse
        return best_feature_idx, best_threshold, best_mse

    def build_tree(self, X, y, depth):
        if depth == self.max_depth or len(y) < self.min_samples_split:
            return np.mean(y)
        feature_idx, threshold, mse = self.find_best_split(X, y)
        if mse == np.inf:
            return np.mean(y)
        X_left, y_left, X_right, y_right = self.split_data(X, y, feature_idx, threshold)
        left_node = self.build_tree(X_left, y_left, depth + 1)
        right_node = self.build_tree(X_right, y_right, depth + 1)
        return {"feature_idx": feature_idx, "threshold": threshold, "left_node": left_node, "right_node": right_node}

    def fit(self, X, y):
        self.tree = self.build_tree(X, y, 0)
    def set_params(self, **params):
          '''function is used to set the values of the attributes of a decision tree object. The function takes a variable 
          number of keyword arguments (**params), 
          which are pairs of attribute names and their corresponding values that should be set for the decision tree object'''
          for param, value in params.items():
            setattr(self, param, value)
            return self

    def predict(self, X):
        def predict_row(row, node):
            if isinstance(node, float):
                return node
            if row[node["feature_idx"]] <= node["threshold"]:
                return predict_row(row, node["left_node"])
            else:
                return predict_row(row, node["right_node"])
        return np.array([predict_row(row, self.tree) for row in X])
    def mean_squared_errorr(self,y_true, y_pred):
   
      # Check if the lengths of both arrays are equal
      if len(y_true) != len(y_pred):
          raise ValueError("Length of y_true and y_pred should be the same.")
      
      # Calculate the squared differences between the true and predicted values
      squared_differences = [(y_true[i] - y_pred[i])**2 for i in range(len(y_true))]
      
      # Calculate the mean of the squared differences
      mse = sum(squared_differences) / len(squared_differences)
      
      return mse

df=df.drop(['sqft_lot15','sqft_above'],axis=1)

#defining dependent and independent variable as y and x
X = df.drop('price',axis=1).values
y = df['price'].values

from sklearn.model_selection import train_test_split


X_train,X_test,Y_train,Y_test=train_test_split(X,y,test_size=.30,random_state=42)

regressor = DecisionTreeRegressor(min_samples_split=4, max_depth=4)
regressor.fit(X_train,Y_train)

y_pred = regressor.predict(X_test)

y_pred

import numpy as np
def mean_squared_error(y_true, y_pred):
    """
    Calculates the mean squared error between y_true and y_pred.
    :param y_true: A list or array of true values
    :param y_pred: A list or array of predicted values
    :return: The mean squared error between y_true and y_pred
    """
    # Get the length of the arrays
    n = len(y_true)
    
    # Calculate the sum of squared differences
    sum_squared_diff = sum((y_true[i] - y_pred[i])**2 for i in range(n))
    
    # Calculate the mean squared error
    mse = sum_squared_diff / n
    
    return mse

mse = mean_squared_error(Y_test, y_pred)
mse

### Create a Pickle file using serialization 
import pickle
pickle_out = open("regressor.pkl","wb")
pickle.dump(regressor, pickle_out)
pickle_out.close()

from sklearn.metrics import mean_squared_error
from random import randint
'''The defaultdict is a subclass of the built-in dict class.
 It overrides one method, __missing__, and adds one writable instance variable, default_factory'''
from collections import defaultdict

# Define the hyperparameters to be tuned
max_depth = [int(x) for x in np.linspace(5, 40, num = 11)]
min_samples_split = [2, 4, 6, 8, 10,12]
min_samples_leaf = [3, 4, 5,8,10,20]

# Define the number of iterations for hyperparameter tuning
n_iter = 10

# Define the number of folds for cross-validation
n_folds = None

# Define a function for k-fold cross-validation
def k_fold_cv(X, y, model, n_folds):
    # Initialize a dictionary to store the cross-validation scores
    cv_scores = defaultdict(list)

    # Divide the data into k folds
    fold_size = len(X) // n_folds
    #fold_starts that contains the starting index of each fold
    fold_starts = [i * fold_size for i in range(n_folds)]
    '''It uses a list comprehension to iterate over the range of n_folds and computes 
    the starting index of each fold as i * fold_size, where i is the fold number'''
    fold_ends = [(i + 1) * fold_size for i in range(n_folds)]
    #fold_ends that contains the ending index of each fold
    fold_ends[-1] = len(X)
    #this line updates the last value in fold_ends to be equal to the length of the dataset X

    # Perform k-fold cross-validation
    for i in range(n_folds):
        # Split the data into training and validation sets
        X_train = np.concatenate([X[:fold_starts[i]], X[fold_ends[i]:]])
        y_train = np.concatenate([y[:fold_starts[i]], y[fold_ends[i]:]])
        X_valid = X[fold_starts[i]:fold_ends[i]]
        y_valid = y[fold_starts[i]:fold_ends[i]]
  
        # Define the hyperparameters to be tuned
        params = {'max_depth': max_depth[randint(0, len(max_depth)-1)],
                  'min_samples_split': min_samples_split[randint(0, len(min_samples_split)-1)],
                  'min_samples_leaf': min_samples_leaf[randint(0, len(min_samples_leaf)-1)]}

        # Train the model with the current hyperparameters
        model.set_params(**params)
        model.fit(X_train, y_train)

        # Evaluate the model on the validation set
        y_pred = model.predict(X_valid)
        mse = mean_squared_error(y_valid, y_pred)

        # Store the cross-validation score
        cv_scores[mse].append(params)

    # Return the best hyperparameters and the corresponding mean squared error
    best_params = cv_scores[min(cv_scores)][0]
    best_mse = min(cv_scores)

    return best_params, best_mse


# Define the decision tree regressor model
dt = DecisionTreeRegressor()

# Perform the hyperparameter tuning using k-fold cross-validation and randomized search
best_params, best_mse = k_fold_cv(X, y, dt, n_folds=10)

# Print the best hyperparameters and the corresponding mean squared error
print("Best hyperparameters:", best_params)
print("Best mean squared error:", best_mse)

df.head()

df.columns

regressor.predict([[2,2000,2004,98178,47,-122,0.299,4,3,0.344,0.275,2,1,4,8,]])

